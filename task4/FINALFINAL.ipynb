{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finalshit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VVXP58eu55G"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "# INSTALL KERAS by running 'conda install -c conda-forge keras' in conda terminal.\n",
        "# Keras Applications are deep learning models.\n",
        "# These models can be used for prediction, feature extraction, and fine-tuning.\n",
        "from tensorflow.keras.preprocessing import image          # For getting image features\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from sklearn.model_selection import train_test_split\n",
        "from zipfile import ZipFile\n",
        "\n",
        "path = '/content/drive/MyDrive/TASK 4/'  # This is path to google drive folder\n",
        "                                         # Change this if you dont run on google colab"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_-bHAM47x1f",
        "outputId": "d83db751-6ecb-485a-9da1-fdd8002c10e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAUhiUXvu6yk"
      },
      "source": [
        "# Unzipping folder with food images\n",
        "\n",
        "with ZipFile(path+'food.zip', 'r') as zipObj:\n",
        "   zipObj.extractall(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0W52_3-nM2d"
      },
      "source": [
        "######### Load train triplets. Separate them randomly into 2 sets- training and validation set\n",
        "\n",
        "train_triplets = '/content/drive/MyDrive/TASK 4/train_triplets.txt'\n",
        "with open(train_triplets, 'r') as file:\n",
        "  triplets = [x for x in file.readlines()]   # Matrix with triplets\n",
        "train_set, validation_set = train_test_split(triplets, test_size=0.1, shuffle = True)  # Separate matrix of triplets into 2 matrices - train and validation set"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJVhsD2du-Q5"
      },
      "source": [
        "########### Additional functions we need for creating dataset ############\n",
        "# Function that reads image into tensorflow dataset\n",
        "# Code from https://github.com/taki0112/Tensorflow-DatasetAPI\n",
        "# Needs to be a function because of the lambda layer in the further code\n",
        "\n",
        "def train_image_processing(img):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)  # 3 channels for RGB\n",
        "    img = tf.cast(img, tf.float32)/ 127.5 - 1\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    # Flipping the train images to ensure that the algorithm \n",
        "    # isn't only trained to recognize images and their mirrors.\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_flip_up_down(img)\n",
        "    return img\n",
        "\n",
        "def test_image_processing(img):  # Same function for test data just without flipping the image\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32)/ 127.5 - 1\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    return img"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjGAomQg1FOD"
      },
      "source": [
        "# Loads image with ids corresponding to the numbers in a triplet\n",
        "# Needs to be a function because of the lambda layer in the further code\n",
        "\n",
        "def load_triplets(triplet):\n",
        "    split_triplet = tf.strings.split(triplet)\n",
        "    anchor = train_image_processing(tf.io.read_file(path+'food/' + split_triplet[0] + '.jpg'))\n",
        "    positive = train_image_processing(tf.io.read_file(path+'food/' + split_triplet[1] + '.jpg'))\n",
        "    negative = train_image_processing(tf.io.read_file(path+'food/' + split_triplet[2] + '.jpg'))\n",
        "    ret_val = tf.stack([anchor, positive, negative], axis=0)\n",
        "    return ret_val, 1\n",
        "\n",
        "def load_test_triplets(triplet):\n",
        "    split_triplet = tf.strings.split(triplet)\n",
        "    anchor = test_image_processing(tf.io.read_file(path+'food/' + split_triplet[0] + '.jpg'))\n",
        "    positive = test_image_processing(tf.io.read_file(path+'food/' + split_triplet[1] + '.jpg'))\n",
        "    negative = test_image_processing(tf.io.read_file(path+'food/' + split_triplet[2] + '.jpg'))\n",
        "    ret_val = tf.stack([anchor, positive, negative], axis=0)\n",
        "    return ret_val"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJSk8-lv7j9I"
      },
      "source": [
        "# Change type of data to datasets, because the model works with this type\n",
        "\n",
        "# Train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_set)\n",
        "train_dataset = train_dataset.map(lambda x: load_triplets(x),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(validation_set)\n",
        "validation_dataset = validation_dataset.map(lambda x: load_triplets(x),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = tf.data.TextLineDataset(path+'test_triplets.txt')\n",
        "test_dataset = test_dataset.map(lambda x: load_test_triplets(x), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(256).prefetch(2)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwAPAG0L7UtA"
      },
      "source": [
        "# Since this is too big for the memory, allocate a buffer;\n",
        "# repeat() will re-initialize the dataset;\n",
        "# batch() will take first batch_size entries and make a batch out of them.\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size = batch_size**2).repeat().batch(batch_size)  \n",
        "validation_dataset = validation_dataset.batch(batch_size)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fur26lY8qz9"
      },
      "source": [
        "############ Create the base model  ############\n",
        "# Code more or less from https://keras.io/api/applications/ and https://keras.io/guides/transfer_learning/\n",
        "\n",
        "inputs = tf.keras.Input(shape=(3, 224, 224, 3))   # Define inputs\n",
        "\n",
        "# Go tru all the images. Use a pre-trained modelin Keras, e.g, VGG16,19 or ResNet.:\n",
        "base_model = tf.keras.applications.DenseNet121(include_top=False, input_shape=(224, 224, 3))\n",
        "#base_model = VGG16(weights='imagenet', include_top=False)\n",
        "#base_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet')\n",
        "\n",
        "base_model.trainable = False         # Freeze all layers in the base model so hyperparameters are the pretraiend ones\n",
        "\n",
        "# Adding layers to neural network\n",
        "embeddings = tf.keras.Sequential()\n",
        "embeddings.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "embeddings.add(tf.keras.layers.Dense(128, activation='linear'))\n",
        "embeddings.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))\n",
        "\n",
        "# Create outputs\n",
        "anchor, positive, negative = inputs[:, 0, ...], inputs[:, 1, ...], inputs[:, 2, ...]\n",
        "anchor_outputs = embeddings(base_model(anchor))\n",
        "positive_outputs = embeddings(base_model(positive))\n",
        "negative_outputs = embeddings(base_model(negative))\n",
        "stacked_outputs = tf.stack([anchor_outputs, positive_outputs, negative_outputs], axis=-1)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdKp2HIW_NjQ"
      },
      "source": [
        "############### Create and compile siamese model\n",
        "\n",
        "# Create custom loss function\n",
        "# Triplet loss: L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
        "def triplet_loss(_, triplet):\n",
        "    anchor, positive, negative = triplet[..., 0], triplet[..., 1], triplet[..., 2]\n",
        "    ret_val = tf.reduce_mean(tf.math.softplus(tf.reduce_sum(tf.square(anchor-positive), 1) - tf.reduce_sum(tf.square(anchor-negative), 1)))\n",
        "    return ret_val\n",
        "\n",
        "# Create custom accuracy function\n",
        "def accuracy(_, triplet):\n",
        "    anchor, positive, negative = triplet[..., 0], triplet[..., 1], triplet[..., 2]\n",
        "    ret_val = tf.reduce_mean(tf.cast(tf.greater_equal(tf.reduce_sum(tf.square(anchor-negative), 1), tf.reduce_sum(tf.square(anchor-positive), 1)), tf.float32))\n",
        "    return ret_val\n",
        "\n",
        "siamese_model = tf.keras.Model(inputs=inputs, outputs=stacked_outputs)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "siamese_model.compile(optimizer=opt, loss=triplet_loss, metrics=[accuracy])\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ArUDAyXA_z6",
        "outputId": "12a50861-2597-4082-ee82-8d58cee84881"
      },
      "source": [
        "########### Fit the model\n",
        "# SKIP THIS IF YOU ALREADY HAVE IMAGE FEATURES BCS RUNNING TAKES A LOT OF TIME\n",
        "\n",
        "#calculated by 0.9*number_of_triplet/32 rounded up (bc batch size = 32)\n",
        "siamese_model.fit(train_dataset, steps_per_epoch = 1674, epochs = 1, validation_data = validation_dataset, validation_steps = 1)\n",
        "\n",
        "# Save weights\n",
        "# out_path = 'trained_model.h5'\n",
        "# siamese_model.save_weights(out_path)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1674/1674 [==============================] - 1660s 974ms/step - loss: 0.5394 - accuracy: 0.7292 - val_loss: 0.3813 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f446f227a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ9-AC9jqGN1"
      },
      "source": [
        "# Load weights if siamese model was already trained\n",
        "# out_path = 'trained_model.h5'\n",
        "# siamese_model.load_weights(out_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy0tgjTdqUnZ",
        "outputId": "254eae31-507d-4390-9efb-d6f60246ffc4"
      },
      "source": [
        "######### Predicting on test data\n",
        "\n",
        "# Getting output types\n",
        "anchor, positive, negative = siamese_model.output[..., 0], siamese_model.output[..., 1], siamese_model.output[..., 2]\n",
        "pos_dist = tf.reduce_sum(tf.square(anchor - positive), 1)\n",
        "neg_dist = tf.reduce_sum(tf.square(anchor - negative), 1)\n",
        "\n",
        "# Creating predict model\n",
        "compared_outputs = tf.cast(tf.greater_equal(neg_dist, pos_dist), tf.int8)   # Cast to int so its 0 or 1\n",
        "predict_model = tf.keras.Model(inputs = siamese_model.inputs, outputs = compared_outputs)\n",
        "\n",
        "num_test_samples = 59544\n",
        "predictions = predict_model.predict(test_dataset, steps=int(numpy.ceil(num_test_samples / 256)), verbose = 1)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "233/233 [==============================] - 1118s 4s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFHs7bjjuBCu"
      },
      "source": [
        "# Save the result to text file\n",
        "\n",
        "numpy.savetxt(path+'output10.txt', predictions, fmt='%i')"
      ],
      "execution_count": 147,
      "outputs": []
    }
  ]
}