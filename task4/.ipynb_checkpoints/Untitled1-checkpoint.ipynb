{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile \n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# You need to INSTALL KERAS by running 'conda install -c conda-forge keras' in conda terminal.\n",
    "# Keras Applications are deep learning models.\n",
    "# These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "from keras.preprocessing import image          # For getting image features\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# For training\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "#garbage collector\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________UNZIPPING IMAGES________________________________________________\n",
    "# Unzip the file with images.\n",
    "#________________________________________________________________________________\n",
    "food_zip = \"food.zip\"                # Specifying zip file name\n",
    "with ZipFile(food_zip, 'r') as zip:  # Opening zip file in read mode\n",
    "    zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________PREPROCESSING IMAGES-GETTING FEATURES___________________________\n",
    "##### SKIP THIS IF YOU ALREADY HAVE IMAGE FEATURES BCS RUNNING TAKES A LOT OF TIME\n",
    "# Go tru all the images. Use a pre-trained modelin Keras, e.g, VGG16,19 or ResNet.\n",
    "# Extract features (vector representation of na image) and put them all in the array.\n",
    "# Code more or less from https://keras.io/api/applications/#vgg16\n",
    "# Running this takes A LOT OF TIME, like an hour on my lap top\n",
    "#________________________________________________________________________________\n",
    "model = VGG16(weights='imagenet', include_top=False)     # Specify pretrained Keras model\n",
    "# Different models we could try:\n",
    "#model = ResNet50(weights='imagenet')                    # ResNet50                 \n",
    "#base_model = VGG19(weights='imagenet')                  # VGG19 \n",
    "#model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "number_of_images = 10000                                 # By openning the file we see that imgs are indexed 0->9999\n",
    "features = torch.empty((number_of_images, 25088))        # Initialize array that contains all the features; \n",
    "                                                         # Feature vector for 1 img has size 25088 (run manually 1 img and see)\n",
    "img_path = './food/'                                     # Folder where images are stored\n",
    "\n",
    "for i in range(number_of_images):                        # Go tru all the images\n",
    "    img_name = str(i).zfill(5) + \".jpg\"                  # Create image file name from an image number\n",
    "    img = image.load_img(img_path+img_name, target_size=(224,224))# Load image; VGG16 input layer takes an image in the size of (224x224x3)\n",
    "    img_data = image.img_to_array(img)                   # Convert image to a matrix\n",
    "    img_data = np.expand_dims(img_data, axis=0)          # inserts a dimension of length 1\n",
    "    img_data = preprocess_input(img_data)                # Preprocesses a tensor or Numpy array encoding a batch of images.\n",
    "\n",
    "    vgg16_feature = model.predict(img_data)              # Extract feature for 1 image\n",
    "    feature_tensor = torch.tensor(vgg16_feature)         # Convert feature to tensor\n",
    "    features[i] = feature_tensor.flatten()               # Add feature to the list of features\n",
    "  \n",
    "torch.save(features, 'features.pt')                      # Save to file so that you dont have to run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________________LOADING PREPROCESSED DATA__________________________________\n",
    "# Loading features from file if the above code was already run.\n",
    "#________________________________________________________________________________\n",
    "features = torch.load('features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________PREPROCESSING SETS OF TRIPLETS___________________________________\n",
    "# Load text files. \n",
    "#_________________________________________________________________________________\n",
    "file_path = 'train_triplets.txt'\n",
    "train_triplets = pd.read_csv(file_path, sep=' ', header=None, dtype=str)     # Load train data\n",
    "train_triplets = torch.tensor(train_triplets.astype(str).astype(int).values) # Converting to tensor\n",
    "\n",
    "file_path = 'test_triplets.txt'                                              # Load test data\n",
    "test_triplets = pd.read_csv(file_path, sep=' ', header=None, dtype=str)\n",
    "test_triplets = torch.tensor(test_triplets.astype(str).astype(int).values)   # Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________PREPARING A MODEL - SIAMESE NETWORK WITH TRIPLET LOSS________________\n",
    "# Most of the code from https://keras.io/examples/vision/siamese_network/\n",
    "#_________________________________________________________________________________\n",
    "\n",
    "def euclidean_distance(xy):                                    #  We had to have 1 input arg bcs of the way the fun is called later\n",
    "    x, y = xy\n",
    "    return K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "                                                               # Other than this we could try contrastive loss\n",
    "def triplet_loss(pos, neg, margin = K.constant(0)):            # L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    loss = pos - neg\n",
    "    loss = K.maximum(loss + margin, K.constant(0))\n",
    "    return loss\n",
    "\n",
    "size_of_features = 25088                                       # Feature vector has length 25088.\n",
    "units = 1024                                                   # I have no idea how to chose this number...                                       \n",
    "\n",
    "#______________\n",
    "# Siamese Network will generate embeddings for each of the images of the triplet. \n",
    "# To do this, we add Dense layers to Keras model. The dense layer is a neural network \n",
    "# layer that is connected deeply, which means each neuron in the dense layer \n",
    "# receives input from all neurons of its previous layer.\n",
    "#______________\n",
    "model_input = kl.Input(shape=25088)\n",
    "model = kl.BatchNormalization()(model_input)                  # Batch normalization mean 0 and the std 1.\n",
    "model = kl.Dense(units*4, activation='relu')(model_input)     # Adding a dense layer.\n",
    "model = kl.Dropout(0.5)(model)                                # The Dropout layer randomly sets input units to 0\n",
    "                                                              # with a frequency of rate at each step during trainings \n",
    "                                                              # time, which helps prevent overfitting.\n",
    "model = kl.BatchNormalization()(model)\n",
    "model = kl.Dense(units*4, activation='relu')(model)\n",
    "model = kl.Dropout(0.5)(model)\n",
    "model = kl.BatchNormalization()(model)\n",
    "model = kl.Dense(units, activation='linear')(model)\n",
    "\n",
    "embedding = Model(model_input, model, name=\"embedding\")       # Groups layers into a network\n",
    "\n",
    "#______________\n",
    "# Input to siamese network: each of the triplet images; It generates\n",
    "# the embeddings, and outputs the distance between the anchor and the positive/negative\n",
    "# embedding.\n",
    "# Anchor is the image in 1st column, positive is 2nd column, negative is 3rd.\n",
    "# To get the distances between anchor and positive/negative embedding, use Lambda layer.\n",
    "# The Lambda layer exists so that arbitrary expressions can be used as a Layer\n",
    "# when constructing models.\n",
    "# tf.keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None, **kwargs\n",
    "#______________\n",
    "anchor_input = kl.Input(shape=size_of_features)               # Create inputs\n",
    "positive_input = kl.Input(shape=size_of_features)\n",
    "negative_input = kl.Input(shape=size_of_features)\n",
    "\n",
    "anchor_output = embedding(anchor_input)                       # Create outputs\n",
    "positive_output = embedding(positive_input)\n",
    "negative_output = embedding(negative_input)\n",
    "\n",
    "positive_distance = kl.Lambda(euclidean_distance)([anchor_output, positive_output])\n",
    "negative_distance = kl.Lambda(euclidean_distance)([anchor_output, negative_output])\n",
    "\n",
    "#______________\n",
    "# Define a model\n",
    "#______________\n",
    "siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=[anchor_output, positive_output, negative_output])\n",
    "siamese_model.compile(optimizer=keras.optimizers.Adam(0.0001), loss=triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5671393280 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-15496b67b7d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 'features' is a huuuge tensor of dimensions 10000 x 25088 so accessing its elements is not so easy....\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# So from here on, i didnt manage to test anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_anchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m                    \u001b[1;31m# Getting features for images that correspond to certain indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mtrain_positive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtrain_negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegative_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5671393280 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "#_________________________MODEL TRAINING__________________________________________\n",
    "# Most of the code from https://keras.io/examples/vision/siamese_network/\n",
    "#_________________________________________________________________________________\n",
    "anchor_indexes = train_triplets[0:56515,0]                    # 95% of data will be used for training\n",
    "positive_indexes = train_triplets[0:56515,1]                  # Getting indexes of 95% of data\n",
    "negative_indexes = train_triplets[0:56515,2]\n",
    " \n",
    "##################THIS IS NOT RUNNING ON MY COMPUTER####################\n",
    "# 'features' is a huuuge tensor of dimensions 10000 x 25088 so accessing its elements is not so easy....\n",
    "# So from here on, i didnt manage to test anything. \n",
    "train_anchors = features[anchor_indexes.type(torch.LongTensor),:]                    # Getting features for images that correspond to certain indexes    \n",
    "train_positive = features[positive_indexes.type(torch.LongTensor),:]\n",
    "train_negative = features[negative_indexes.type(torch.LongTensor),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5671393280 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c135bf950556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# So from here on, i didnt manage to test anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_anchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m                    \u001b[1;31m# Getting features for images that correspond to certain indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_positive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mtrain_negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegative_indexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5671393280 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "anchor_indexes = train_triplets[56515:, 0]\n",
    "validation_anchors = features[anchor_indexes,:]    # 5% of data will be used for validation\n",
    "anchor_indexes = ''\n",
    "\n",
    "positive_indexes = train_triplets[56515:,1]\n",
    "validation_positive = features[positive_indexes,:]\n",
    "positive_indexes =''\n",
    "\n",
    "negative_indexes = train_triplets[56515:,2]\n",
    "validation_negative = features[negative_indexes,:]\n",
    "validation_negative =''\n",
    "\n",
    "siamese_model.fit([train_anchors, train_positive, train_negative], epochs=10, validation_data=[validation_anchors, validation_positive, validation_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_indexes.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Get test data the same way as above this last par which is not working \n",
    "# - Predict distances for test data:  distances = siamese_model.model.predict([anchors_test, positives_test, negatives_test])\n",
    "# - Getting labels from distances : \n",
    "# for d in distances:\n",
    "#        if d[0] > d[1]:       #d[0] will be positive distance, d[1] negative\n",
    "#            results[i] = 0    \n",
    "#        else:\n",
    "#            results[i] = 1\n",
    "#        i+=1\n",
    "\n",
    "# - Saving to file: np.savetxt('output.txt', results.astype(int), fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
