{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trenddetector(list_of_index, array_of_data, order=1): # This function returns a number that describes the trend of the data. 0 means constant\n",
    "                                                          # trend or that all values are NaN. Higher positive number means higher incresing trend.\n",
    "                                                          # Similarly negative numbers correspond to decreasing trend.\n",
    "    warnings.filterwarnings(\"ignore\") # This function gives tons of warnings because polyfit thinks that function order 1 is too low.\n",
    "                                      # But we don't care that the fit is good, we just want linear function in order to see the slope.\n",
    "\n",
    "    idx = np.isfinite(array_of_data) # Getting rid of NaN values because polyfit cannot work with them\n",
    "    if array_of_data[idx].size ==0:  # If all values were NaN, return 0\n",
    "        return 0\n",
    "    result = np.polyfit(np.array(list_of_index)[idx], array_of_data[idx], order)  # Trend will be actually slope of linear function fitted tru data\n",
    "    slope = result[-2]\n",
    "    return round(float(slope), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    number_of_hours = 12\n",
    "    number_of_patients = int(data.shape[0]/number_of_hours)\n",
    "    statistical_features = int(data.shape[1])-3  # Number of columns - pid - age - time\n",
    "\n",
    "    # Transforming train features into an array where each element is one patient\n",
    "    train_features_array = np.array(data)\n",
    "    patients = np.vsplit(train_features_array , number_of_patients)\n",
    "    patients_array = np.array(patients)\n",
    "    \n",
    "    # We are creating a new matrix (Phi) that contains statistical features for every patient: pid, age, features: mean, var, max, min, median\n",
    "    features = np.zeros([number_of_patients, 2 + 6*statistical_features]) # no. of patients, no. of features* no. of columns names(age, calcium, bolirubin...)\n",
    "    # columns are: pid, age, mean(of all featuers), var, max, min, median, trend\n",
    "    \n",
    "    features[:,0] = patients_array[:,0,0] # 1st column is Patient id\n",
    "    features[:,1] = patients_array[:,0,2] # 2nd column is Age\n",
    "    \n",
    "    means = np.nanmean(patients_array[:, :, 3:], axis=1) # rest of the columns are means, var... of all values\n",
    "    features[:,2:statistical_features+2] = means\n",
    "    \n",
    "    variances = np.nanvar(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,statistical_features+2:2*statistical_features+2] = variances\n",
    "    \n",
    "    maxs = np.nanmax(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,2*statistical_features+2:3*statistical_features+2] = maxs\n",
    "    \n",
    "    mins = np.nanmin(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,3*statistical_features+2:4*statistical_features+2] = mins\n",
    "    \n",
    "    medians = np.nanmedian(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,4*statistical_features+2:5*statistical_features+2] = medians\n",
    "    \n",
    "    trends = np.zeros([patients_array[:, :,3:].shape[0],patients_array[:, :,3:].shape[2]])\n",
    "    for i in range(patients_array[:, :,3:].shape[0]):\n",
    "        for j in range(patients_array[:, :,3:].shape[2]-3):\n",
    "            trends[i,j] = trenddetector([1,2,3,4,5,6,7,8,9,10,11,12], patients_array[i, :, 3+j])\n",
    "    features[:,5*statistical_features+2:6*statistical_features+2] = trends\n",
    "    \n",
    "    # Replacing the NaN values with the mean of all patients\n",
    "    all_means = np.nanmean(features, axis=0)  # Calculate the mean of each features of all patients\n",
    "    \n",
    "    # find indexes where NaNs are and replace them by the column mean\n",
    "    indexes = np.where(np.isnan(features))\n",
    "    features[indexes] = np.take(all_means, indexes[1])\n",
    "    \n",
    "    # imp = SimpleImputer(missing_values=np.nan, strategy='mean') # I put imputer here just because they mentioned it in the task description\n",
    "    # features = imp.fit_transform(features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files\n",
    "train_features = pd.read_csv(\"train_features.csv\") # training X\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")     # training Y\n",
    "test_features = pd.read_csv(\"test_features.csv\")   # testing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b92f09493c62>:18: RuntimeWarning: Mean of empty slice\n",
      "  means = np.nanmean(patients_array[:, :, 3:], axis=1) # rest of the columns are means, var... of all values\n",
      "<ipython-input-3-b92f09493c62>:21: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  variances = np.nanvar(patients_array[:, :, 3:], axis=1);\n",
      "<ipython-input-3-b92f09493c62>:24: RuntimeWarning: All-NaN slice encountered\n",
      "  maxs = np.nanmax(patients_array[:, :, 3:], axis=1);\n",
      "<ipython-input-3-b92f09493c62>:27: RuntimeWarning: All-NaN slice encountered\n",
      "  mins = np.nanmin(patients_array[:, :, 3:], axis=1);\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing both train and test features: creating an array of patients, creating statistical features (mean, var, max..)\n",
    "# for all features for each patient, getting rid of NaN values with filling them with means.\n",
    "train_X = preprocess(train_features)\n",
    "test_X = preprocess(test_features)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "train_X[:,2:] = scaler.fit_transform(train_X[:,2:])  # Skipping pid and age\n",
    "\n",
    "test_X[:,2:] = scaler.fit_transform(test_X[:,2:])\n",
    "\n",
    "# Exporting train_X into csv file for better observing\n",
    "df = pd.DataFrame(train_X)\n",
    "df.to_csv('train_X2.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore these warnings. It reports that somewhere in calculating mean/var.. of all values we get NaN for columns that contist only of NaN's. Afterwards we were dealing with these NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts 1 & 2 (They're basically the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtasks 1 & 2\n",
    "train_Y = np.array(train_labels.loc[: , \"pid\":\"LABEL_Sepsis\"])\n",
    "no_columns = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score:  0.9310867893468737\n",
      "ROC score:  0.7985395173154021\n",
      "ROC score:  0.7450505941096053\n",
      "ROC score:  0.7489664875085307\n",
      "ROC score:  0.7447147435532003\n",
      "ROC score:  0.8128146442454176\n",
      "ROC score:  0.8953945340935183\n",
      "ROC score:  0.8350271912216016\n",
      "ROC score:  0.7594086787475302\n",
      "ROC score:  0.9338525708115661\n"
     ]
    }
   ],
   "source": [
    "# We are using cross validation to evaluate different models.\n",
    "# We are using ROC scoring bcs it is how the performance of the model should be evaulated.\n",
    "\n",
    "classifier = HistGradientBoostingClassifier()  # Uncomment the classifier you want to try\n",
    "#classifier = GradientBoostingClassifier() #This one doesn't run on my computer\n",
    "#classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Task 1\n",
    "for i in range(1, no_columns-1) :  # We are calculating ROC scores with cross validation for all tests, one by one.\n",
    "    score = cross_val_score(classifier, train_X, train_Y[:, i], cv=10, scoring='roc_auc')\n",
    "    print ('ROC score: ', score.mean())   # 'score' will be an array that contains 10 scores \n",
    "                                          # (from 10 validations since cv=10). We are reporting the mean of those.\n",
    "# ROC score should be close to 1 and ideally 1. (1 will probably mean overfitting though). So try to run this code for different\n",
    "# classifiers and chose the one that gives the highest (reasonable) ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-96f8b950da0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_columns\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m  \u001b[1;31m# We are fitting columns (medical tests) one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0moutput_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpredictions_part_1_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Adding the column of the predicted labels into the resulting matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    204\u001b[0m         self.bin_mapper_ = _BinMapper(n_bins=n_bins,\n\u001b[0;32m    205\u001b[0m                                       random_state=self._random_seed)\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mX_binned_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bin_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mX_binned_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bin_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36m_bin_data\u001b[1;34m(self, X, is_training_data)\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_training_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mX_binned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbin_mapper_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# F-aligned array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0mX_binned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbin_mapper_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# F-aligned array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_DTYPE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mmax_bins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_bins\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         self.bin_thresholds_ = _find_binning_thresholds(\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_bins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             random_state=self.random_state)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py\u001b[0m in \u001b[0;36m_find_binning_thresholds\u001b[1;34m(data, max_bins, subsample, random_state)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mcol_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# ignore missing values when computing bin thresholds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mmissing_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mcol_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we use the classifier chosen above in order to predict labels for the test sets.\n",
    " \n",
    "predictions_part_1_2 = np.zeros([test_X.shape[0], no_columns])\n",
    "\n",
    "for i in range(1, no_columns-1) :  # We are fitting columns (medical tests) one by one\n",
    "    classifier.fit(train_X, train_Y[:, i])\n",
    "    output_proba = classifier.predict_proba(test_X) \n",
    "    predictions_part_1_2[:, i] = output_proba[:,1]  # Adding the column of the predicted labels into the resulting matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score:  0.6962300577081816\n"
     ]
    }
   ],
   "source": [
    "# Task 2 - separated only because another classifier gives better solution\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "score = cross_val_score(classifier, train_X, train_Y[:, 11], cv=10, scoring='roc_auc')\n",
    "print ('ROC score: ', score.mean()) \n",
    "\n",
    "classifier.fit(train_X, train_Y[:, 11])\n",
    "output_proba = classifier.predict_proba(test_X) \n",
    "predictions_part_1_2[:, 11] = output_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_part_1_2[:, 0] = test_X[:, 0].astype(int) # 1st column is just pid-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtask 3\n",
    "train_Y = np.array(train_labels.loc[: , \"LABEL_RRate\":\"LABEL_Heartrate\"])\n",
    "no_columns = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.4324244766452422\n",
      "R2 score:  0.6320441078233323\n",
      "R2 score:  0.3941437405822451\n",
      "R2 score:  0.6605121212215167\n"
     ]
    }
   ],
   "source": [
    "# We are using cross validation to evaluate different models.\n",
    "# We are using R2 scoring bcs it is how the performance of the model should be evaulated.\n",
    "\n",
    "# regressor = RandomForestRegressor(random_state=42)   # Uncomment the redressor you want to try\n",
    "regressor = HistGradientBoostingRegressor()  \n",
    "\n",
    "for i in range(no_columns) :  # We are calculating ROC scores with cross validation for all tests, one by one.\n",
    "    score = cross_val_score(regressor, train_X, train_Y[:, i], cv=5, scoring='r2')\n",
    "    print ('R2 score: ', score.mean())   \n",
    "    \n",
    "# R2 score should be close to 1 and ideally 1. (1 will probably mean overfitting though). So try to run this code for different\n",
    "# classifiers and chose the one that gives the highest (reasonable) R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the regressor chosen above in order to predict labels for the test sets.\n",
    "\n",
    "predictions_part_3 = np.zeros([test_X.shape[0], no_columns])\n",
    "for i in range(no_columns) :\n",
    "    regressor.fit(train_X, train_Y[:, i])\n",
    "    output = regressor.predict(test_X)\n",
    "    predictions_part_3[:, i] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate((predictions_part_1_2,predictions_part_3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output, columns = ['pid','LABEL_BaseExcess','LABEL_Fibrinogen','LABEL_AST','LABEL_Alkalinephos','LABEL_Bilirubin_total','LABEL_Lactate','LABEL_TroponinI','LABEL_SaO2','LABEL_Bilirubin_direct','LABEL_EtCO2','LABEL_Sepsis','LABEL_RRate','LABEL_ABPm','LABEL_SpO2','LABEL_Heartrate'])\n",
    "\n",
    "df.to_csv('prediction.csv', index=False, float_format='%.3f')  # I left this here for easier overview\n",
    "df.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
