{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trenddetector(list_of_index, array_of_data, order=1): # This function returns a number that describes the trend of the data. 0 means constant\n",
    "                                                          # trend or that all values are NaN. Higher positive number means higher incresing trend.\n",
    "                                                          # Similarly negative numbers correspond to decreasing trend.\n",
    "    warnings.filterwarnings(\"ignore\") # This function gives tons of warnings because polyfit thinks that function order 1 is too low.\n",
    "                                      # But we don't care that the fit is good, we just want linear function in order to see the slope.\n",
    "\n",
    "    idx = np.isfinite(array_of_data) # Getting rid of NaN values because polyfit cannot work with them\n",
    "    if array_of_data[idx].size ==0:  # If all values were NaN, return 0\n",
    "        return 0\n",
    "    result = np.polyfit(np.array(list_of_index)[idx], array_of_data[idx], order)  # Trend will be actually slope of linear function fitted tru data\n",
    "    slope = result[-2]\n",
    "    return round(float(slope), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    number_of_hours = 12\n",
    "    number_of_patients = int(data.shape[0]/number_of_hours)\n",
    "    statistical_features = int(data.shape[1])-3  # Number of columns - pid - age - time\n",
    "\n",
    "    # Transforming train features into an array where each element is one patient\n",
    "    train_features_array = np.array(data)\n",
    "    patients = np.vsplit(train_features_array , number_of_patients)\n",
    "    patients_array = np.array(patients)\n",
    "    \n",
    "    # We are creating a new matrix (Phi) that contains statistical features for every patient: pid, age, features: mean, var, max, min, median\n",
    "    features = np.zeros([number_of_patients, 2 + 6*statistical_features]) # no. of patients, no. of features* no. of columns names(age, calcium, bolirubin...)\n",
    "    # columns are: pid, age, mean(of all featuers), var, max, min, median, trend\n",
    "    \n",
    "    features[:,0] = patients_array[:,0,0] # 1st column is Patient id\n",
    "    features[:,1] = patients_array[:,0,2] # 2nd column is Age\n",
    "    \n",
    "    means = np.nanmean(patients_array[:, :, 3:], axis=1) # rest of the columns are means, var... of all values\n",
    "    features[:,2:statistical_features+2] = means\n",
    "    \n",
    "    variances = np.nanvar(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,statistical_features+2:2*statistical_features+2] = variances\n",
    "    \n",
    "    maxs = np.nanmax(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,2*statistical_features+2:3*statistical_features+2] = maxs\n",
    "    \n",
    "    mins = np.nanmin(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,3*statistical_features+2:4*statistical_features+2] = mins\n",
    "    \n",
    "    medians = np.nanmedian(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,4*statistical_features+2:5*statistical_features+2] = medians\n",
    "    \n",
    "    trends = np.zeros([patients_array[:, :,3:].shape[0],patients_array[:, :,3:].shape[2]])\n",
    "    for i in range(patients_array[:, :,3:].shape[0]):\n",
    "        for j in range(patients_array[:, :,3:].shape[2]-3):\n",
    "            trends[i,j] = trenddetector([1,2,3,4,5,6,7,8,9,10,11,12], patients_array[i, :, 3+j])\n",
    "    features[:,5*statistical_features+2:6*statistical_features+2] = trends\n",
    "    \n",
    "    # Replacing the NaN values with the mean of all patients\n",
    "    all_means = np.nanmean(features, axis=0)  # Calculate the mean of each features of all patients\n",
    "    \n",
    "    # find indexes where NaNs are and replace them by the column mean\n",
    "    indexes = np.where(np.isnan(features))\n",
    "    features[indexes] = np.take(all_means, indexes[1])\n",
    "    \n",
    "    # imp = SimpleImputer(missing_values=np.nan, strategy='mean') # I put imputer here just because they mentioned it in the task description\n",
    "    # features = imp.fit_transform(features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files\n",
    "train_features = pd.read_csv(\"train_features.csv\") # training X\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")     # training Y\n",
    "test_features = pd.read_csv(\"test_features.csv\")   # testing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b92f09493c62>:18: RuntimeWarning: Mean of empty slice\n",
      "  means = np.nanmean(patients_array[:, :, 3:], axis=1) # rest of the columns are means, var... of all values\n",
      "<ipython-input-3-b92f09493c62>:21: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  variances = np.nanvar(patients_array[:, :, 3:], axis=1);\n",
      "<ipython-input-3-b92f09493c62>:24: RuntimeWarning: All-NaN slice encountered\n",
      "  maxs = np.nanmax(patients_array[:, :, 3:], axis=1);\n",
      "<ipython-input-3-b92f09493c62>:27: RuntimeWarning: All-NaN slice encountered\n",
      "  mins = np.nanmin(patients_array[:, :, 3:], axis=1);\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing both train and test features: creating an array of patients, creating statistical features (mean, var, max..)\n",
    "# for all features for each patient, getting rid of NaN values with filling them with means.\n",
    "train_X = preprocess(train_features)\n",
    "test_X = preprocess(test_features)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "train_X[:,2:] = scaler.fit_transform(train_X[:,2:])  # Skipping pid and age\n",
    "\n",
    "test_X[:,2:] = scaler.fit_transform(test_X[:,2:])\n",
    "\n",
    "# Exporting train_X into csv file for better observing\n",
    "df = pd.DataFrame(train_X)\n",
    "df.to_csv('train_X2.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore these warnings. It reports that somewhere in calculating mean/var.. of all values we get NaN for columns that contist only of NaN's. Afterwards we were dealing with these NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts 1 & 2 (They're basically the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtasks 1 & 2\n",
    "train_Y = np.array(train_labels.loc[: , \"pid\":\"LABEL_Sepsis\"])\n",
    "no_columns = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score:  nan\n",
      "ROC score:  nan\n",
      "ROC score:  nan\n"
     ]
    }
   ],
   "source": [
    "# We are using cross validation to evaluate different models.\n",
    "# We are using ROC scoring bcs it is how the performance of the model should be evaulated.\n",
    "\n",
    "# classifier = HistGradientBoostingClassifier()  # Uncomment the classifier you want to try\n",
    "#classifier = GradientBoostingClassifier() #This one doesn't run on my computer\n",
    "classifier = SGDClassifier()\n",
    "\n",
    "for i in range(1, no_columns) :  # We are calculating ROC scores with cross validation for all tests, one by one.\n",
    "    score = cross_val_score(classifier, train_X, train_Y[:, i], cv=10, scoring='roc_auc')\n",
    "    print ('ROC score: ', score.mean())   # 'score' will be an array that contains 10 scores \n",
    "                                          # (from 10 validations since cv=10). We are reporting the mean of those.\n",
    "# ROC score should be close to 1 and ideally 1. (1 will probably mean overfitting though). So try to run this code for different\n",
    "# classifiers and chose the one that gives the highest (reasonable) ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cde10e9d116c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_columns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m  \u001b[1;31m# We are fitting columns (medical tests) one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0moutput_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpredictions_part_1_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Adding the column of the predicted labels into the resulting matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    354\u001b[0m             \u001b[1;31m# Build `n_trees_per_iteration` trees.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_trees_per_iteration_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 grower = TreeGrower(\n\u001b[0m\u001b[0;32m    357\u001b[0m                     \u001b[0mX_binned_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                     \u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_binned, gradients, hessians, max_leaf_nodes, max_depth, min_samples_leaf, min_gain_to_split, n_bins, n_bins_non_missing, has_missing_values, monotonic_cst, l2_regularization, min_hessian_to_split, shrinkage)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_compute_hist_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m  \u001b[1;31m# time spent computing histograms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_apply_split_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m  \u001b[1;31m# time spent splitting nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_intilialize_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians_are_constant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36m_intilialize_root\u001b[1;34m(self, gradients, hessians, hessians_are_constant)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         self.root.histograms = self.histogram_builder.compute_histograms_brute(\n\u001b[0m\u001b[0;32m    335\u001b[0m             self.root.sample_indices)\n\u001b[0;32m    336\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_best_split_and_push\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we use the classifier chosen above in order to predict labels for the test sets.\n",
    " \n",
    "predictions_part_1_2 = np.zeros([test_X.shape[0], no_columns])\n",
    "\n",
    "for i in range(1, no_columns) :  # We are fitting columns (medical tests) one by one\n",
    "    \n",
    "    classifier.fit(train_X, train_Y[:, i])\n",
    "    output_proba = classifier.predict_proba(test_X) \n",
    "    predictions_part_1_2[:, i] = output_proba[:,1]  # Adding the column of the predicted labels into the resulting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_part_1_2[:, 0] = test_X[:, 0].astype(int) # 1st column is just pid-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtask 3\n",
    "train_Y = np.array(train_labels.loc[: , \"LABEL_RRate\":\"LABEL_Heartrate\"])\n",
    "no_columns = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.4315989184837205\n",
      "R2 score:  0.6319116529937765\n",
      "R2 score:  0.3978706887787095\n",
      "R2 score:  0.6599542701466206\n"
     ]
    }
   ],
   "source": [
    "# We are using cross validation to evaluate different models.\n",
    "# We are using R2 scoring bcs it is how the performance of the model should be evaulated.\n",
    "\n",
    "# regressor = SGDRegressor(max_iter = 1000)   # Uncomment the redressor you want to try\n",
    "regressor = HistGradientBoostingRegressor()  \n",
    "\n",
    "for i in range(no_columns) :  # We are calculating ROC scores with cross validation for all tests, one by one.\n",
    "    score = cross_val_score(regressor, train_X, train_Y[:, i], cv=5, scoring='r2')\n",
    "    print ('R2 score: ', score.mean())   \n",
    "    \n",
    "# R2 score should be close to 1 and ideally 1. (1 will probably mean overfitting though). So try to run this code for different\n",
    "# classifiers and chose the one that gives the highest (reasonable) R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the regressor chosen above in order to predict labels for the test sets.\n",
    "\n",
    "predictions_part_3 = np.zeros([test_X.shape[0], no_columns])\n",
    "for i in range(no_columns) :\n",
    "    regressor.fit(train_X, train_Y[:, i])\n",
    "    output = regressor.predict(test_X)\n",
    "    predictions_part_3[:, i] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate((predictions_part_1_2,predictions_part_3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output, columns = ['pid','LABEL_BaseExcess','LABEL_Fibrinogen','LABEL_AST','LABEL_Alkalinephos','LABEL_Bilirubin_total','LABEL_Lactate','LABEL_TroponinI','LABEL_SaO2','LABEL_Bilirubin_direct','LABEL_EtCO2','LABEL_Sepsis','LABEL_RRate','LABEL_ABPm','LABEL_SpO2','LABEL_Heartrate'])\n",
    "\n",
    "df.to_csv('prediction.csv', index=False, float_format='%.3f')  # I left this here for easier overview\n",
    "df.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
