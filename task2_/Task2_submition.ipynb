{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    number_of_hours = 12\n",
    "    number_of_patients = int(data.shape[0]/number_of_hours)\n",
    "    statistical_features = int(data.shape[1])-3  # Number of columns - pid - age - time\n",
    "\n",
    "    # Transforming train features into an array where each element is one patient\n",
    "    train_features_array = np.array(data)\n",
    "    patients = np.vsplit(train_features_array , number_of_patients)\n",
    "    patients_array = np.array(patients)\n",
    "    \n",
    "    # We are creating a new matrix (Phi) that contains statistical features for every patient: pid, age, features: mean, var, max, min, median\n",
    "    features = np.zeros([number_of_patients, 2+5*statistical_features]) # no. of patients, no. of features* no. of columns names(age, calcium, bolirubin...)\n",
    "    # columns are: pid, age, mean(of all featuers), var, max, min, median\n",
    "    \n",
    "    features[:,0] = patients_array[:,0,0] # 1st column is Patient id\n",
    "    features[:,1] = patients_array[:,0,2] # 2nd column is Age\n",
    "    \n",
    "    means = np.nanmean(patients_array[:, :, 3:], axis=1) # rest of the columns are means, var... of all values\n",
    "    features[:,2:statistical_features+2] = means\n",
    "    \n",
    "    variances = np.nanvar(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,statistical_features+2:2*statistical_features+2] = variances\n",
    "    \n",
    "    maxs = np.nanmax(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,2*statistical_features+2:3*statistical_features+2] = maxs\n",
    "    \n",
    "    mins = np.nanmin(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,3*statistical_features+2:4*statistical_features+2] = mins\n",
    "    \n",
    "    medians = np.nanmedian(patients_array[:, :, 3:], axis=1);\n",
    "    features[:,4*statistical_features+2:5*statistical_features+2] = medians\n",
    "    \n",
    "    # Replacing the NaN values with the mean of all patients\n",
    "    all_medians = np.nanmedian(features, axis=0)  # Calculate the mean of each features of all patients\n",
    "    \n",
    "    # find indexes where NaNs are and replace them by the column mean\n",
    "    indexes = np.where(np.isnan(features))\n",
    "    features[indexes] = np.take(all_medians, indexes[1])\n",
    "    \n",
    "    # imp = SimpleImputer(missing_values=np.nan, strategy='mean') # I put imputer here just because they mentioned it in the task description\n",
    "    # features = imp.fit_transform(features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files\n",
    "train_features = pd.read_csv(\"train_features.csv\") # training X\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")     # training Y\n",
    "test_features = pd.read_csv(\"test_features.csv\")   # testing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\paulm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "C:\\Users\\paulm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\paulm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\paulm\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing both train and test features: creating an array of patients, creating statistical features (mean, var, max..)\n",
    "# for all features for each patient, getting rid of NaN values with filling them with means.\n",
    "train_X = preprocess(train_features)\n",
    "test_X = preprocess(test_features)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "train_X[:,2:] = scaler.fit_transform(train_X[:,2:])  # Skipping pid and age\n",
    "\n",
    "test_X[:,2:] = scaler.fit_transform(test_X[:,2:])\n",
    "\n",
    "# Exporting train_X into csv file for better observing\n",
    "df = pd.DataFrame(train_X)\n",
    "df.to_csv('train_X2.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore these warnings. It reports that somewhere in calculating mean/var.. of all values we get NaN for columns that contist only of NaN's. Afterwards we were dealing with these NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts 1 & 2 (They're basically the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtasks 1 & 2\n",
    "train_Y = np.array(train_labels.loc[: , \"pid\":\"LABEL_Sepsis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9699886081049165\n",
      "0.9752133317094954\n",
      "0.8916091407872057\n",
      "0.8719068271179388\n",
      "0.8511676330829121\n",
      "0.8916043977088014\n",
      "0.965048897529664\n",
      "0.9136123635212411\n",
      "0.9481187411385505\n",
      "0.9933098589333478\n",
      "0.9328472588356836\n"
     ]
    }
   ],
   "source": [
    "no_columns = 12\n",
    "predictions_part_1_2 = np.zeros([test_X.shape[0], no_columns])\n",
    "\n",
    "for i in range(1, no_columns) :  # We are fitting columns (medical tests) one by one\n",
    "    \n",
    "    model.fit(train_X, train_Y[:, i])\n",
    "    output_proba = model.predict_proba(test_X) \n",
    "    predictions_part_1_2[:, i] = output_proba[:,1]\n",
    "    \n",
    "    # ROC for analyzing\n",
    "    print(roc_auc_score(train_Y[:, i], model.predict_proba(train_X)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9697331 , 0.0302669 ],\n",
       "       [0.97776384, 0.02223616],\n",
       "       [0.96662617, 0.03337383],\n",
       "       ...,\n",
       "       [0.97403857, 0.02596143],\n",
       "       [0.96337587, 0.03662413],\n",
       "       [0.93770191, 0.06229809]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 1.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.000e+01, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.000e+02, 1.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [9.996e+03, 1.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [9.998e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [9.999e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_part_1_2[:, 0] = test_X[:, 0].astype(int) # 1st columns is just pid-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.58154090e-01, 3.83518902e-01, ...,\n",
       "        1.98527731e-01, 1.60926542e-03, 7.43454886e-02],\n",
       "       [1.00010000e+04, 2.34072772e-01, 2.51222503e-02, ...,\n",
       "        2.16825190e-02, 2.03836107e-02, 2.72148618e-02],\n",
       "       [1.00030000e+04, 1.50609330e-02, 1.53320967e-02, ...,\n",
       "        1.22769833e-02, 2.04152165e-02, 4.57730869e-02],\n",
       "       ...,\n",
       "       [9.99200000e+03, 3.78262592e-01, 2.28235214e-02, ...,\n",
       "        1.24983809e-02, 4.33613761e-03, 4.94861161e-02],\n",
       "       [9.99400000e+03, 9.76032622e-01, 2.79909621e-01, ...,\n",
       "        4.22132366e-02, 2.23498158e-03, 5.82144119e-02],\n",
       "       [9.99700000e+03, 7.64669575e-01, 1.09865006e-02, ...,\n",
       "        1.37837371e-02, 1.33478952e-03, 5.71158379e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_part_1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data we need for subtask 3\n",
    "train_Y = np.array(train_labels.loc[: , \"LABEL_RRate\":\"LABEL_Heartrate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressor = SGDRegressor(max_iter = 1000)\n",
    "regressor = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_columns = 4\n",
    "predictions_part_3 = np.zeros([test_X.shape[0], no_columns])\n",
    "for i in range(no_columns) :\n",
    "    regressor.fit(train_X, train_Y[:, i])\n",
    "    output = regressor.predict(test_X)\n",
    "    predictions_part_3[:, i] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate((predictions_part_1_2,predictions_part_3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output, columns = ['pid','LABEL_BaseExcess','LABEL_Fibrinogen','LABEL_AST','LABEL_Alkalinephos','LABEL_Bilirubin_total','LABEL_Lactate','LABEL_TroponinI','LABEL_SaO2','LABEL_Bilirubin_direct','LABEL_EtCO2','LABEL_Sepsis','LABEL_RRate','LABEL_ABPm','LABEL_SpO2','LABEL_Heartrate'])\n",
    "\n",
    "df.to_csv('prediction.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.58154090e-01, 3.83518902e-01, ...,\n",
       "        1.98527731e-01, 1.60926542e-03, 7.43454886e-02],\n",
       "       [1.00010000e+04, 2.34072772e-01, 2.51222503e-02, ...,\n",
       "        2.16825190e-02, 2.03836107e-02, 2.72148618e-02],\n",
       "       [1.00030000e+04, 1.50609330e-02, 1.53320967e-02, ...,\n",
       "        1.22769833e-02, 2.04152165e-02, 4.57730869e-02],\n",
       "       ...,\n",
       "       [9.99200000e+03, 3.78262592e-01, 2.28235214e-02, ...,\n",
       "        1.24983809e-02, 4.33613761e-03, 4.94861161e-02],\n",
       "       [9.99400000e+03, 9.76032622e-01, 2.79909621e-01, ...,\n",
       "        4.22132366e-02, 2.23498158e-03, 5.82144119e-02],\n",
       "       [9.99700000e+03, 7.64669575e-01, 1.09865006e-02, ...,\n",
       "        1.37837371e-02, 1.33478952e-03, 5.71158379e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_part_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
